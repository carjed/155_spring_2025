---
title: "03/04. Introduction to simple linear regression"
subtitle: "Notes and in-class exercises"
format: 
  html:
    embed-resources: true
    toc: true
eval: false # change this to "true" if you want to render a full HTML report for this activity     
---

-   Download the `.qmd` file for this activity [here](../activity_templates/03_slr_introduction.qmd).

# Notes

## Learning goals

By the end of this lesson, you should be able to:

-   Visualize and describe the relationship between two **quantitative** variables using a **scatterplot**
-   Describe/identify **weak** / **strong**, and **positive** / **negative** correlation from a point cloud
-   Differentiate between a **response** / **outcome variable** and a **predictor** / **explanatory variable**
-   Write a **model formula** and R code for a simple linear regression model with a quantitative predictor
-   Interpret the **intercept** and **slope** coefficients in a simple linear regression model with a quantitative predictor
-   Compute and interpret **expected / predicted / fitted values** and **residuals** from a linear regression model formula

## Readings and videos

-   Reading: Sections 2.8, 3.1-3.3, 3.6 in the [STAT 155 Notes](https://mac-stat.github.io/Stat155Notes/)
-   Videos:
    -   [Summarizing the Relationships between Two Quantitative Variables](https://voicethread.com/share/14942266/) (Time: 12:12)
    -   [Introduction to Linear Models](https://voicethread.com/share/14927659/) (Time: 10:57)
    -   [Method of Least Squares](https://voicethread.com/share/14980221/) (Time: 5:10)
    -   [Interpretation of Intercept and Slope](https://voicethread.com/share/15148106/) (Time: 11:09)
    -   [R Code for Fitting a Linear Model](https://voicethread.com/share/14928147/) (Time: 11:07)

**File organization:** Save this file in the "Activities" subfolder of your "STAT155" folder.

# Exercises

**Context:** Today we'll explore data from the results of weightlifting competitions over several decades. The data originally came from [Kaggle](https://www.kaggle.com/open-powerlifting/powerlifting-database) and [OpenPowerlifting](https://www.openpowerlifting.org/). Our main goal will be to explore the relationship between strength-to-weight ratio (SWR) and body weight. Read in the data below.

```{r warning=FALSE, message=FALSE}
# Load packages and import data
library(tidyverse)

lifts <- read_csv("https://mac-stat.github.io/data/powerlifting.csv")
```

## Exercise 1: Get to know the data

a.  Create a new code chunk by clicking the green "C" button with a green + sign in the top right of the menu bar. In this code chunk, use an appropriate function to look at the first few rows of the data.

b.  Create a new code chunk, and use an appropriate function to summarize how much data we have (in terms of number of cases and number of variables).

c.  What does a case represent?

d.  Navigate to the [FAQ page](https://www.openpowerlifting.org/faq) and read the response to the "How does this site work? Do you just download results from the federations?" question. What do you learn about data quality and completeness from this response?

## Exercise 2: Modifying our dataframe

### 2a: mutating

A popular variable of interest in weightlifting is the Strength-to-Weight ratio (SWR), defined as `TotalKg`/`BodyweightKg`. We can use the `mutate()` function from the `dplyr` package to create a new `SWR` variable in our dataframe using the following code:

```{r}
# The %>% is called a "pipe" and feeds what comes before it
# into what comes after (lifts data is "fed into" the mutate() function).
# When creating a new variable, we often reassign the data frame to itself,
# which updates the existing columns in lifts with the additional "new" column(s)
# in lifts!
lifts <- lifts %>% 
    mutate(SWR = TotalKg/BodyweightKg)
```

### 2b: filtering

Another common task you'll see in R is "filtering" a dataframe to only contain observations that match some criteria. Here, we'll filter our dataframe to include only people age 18 or higher from the USA:

```{r}
# The %>% is called a "pipe" and feeds what comes before it
# into what comes after (lifts data is "piped into" the filter() function)
lifts <- lifts %>% 
    filter(Country == "USA", Age>=18)
```

Let's look at how many observations are in our new dataframeâ€“how does this compare to the number of observations in the full `lifts` dataframe that you found in exercise 1b?

```{r}
nrow(lifts)
```

::: {.callout-note collapse="false"}
### Reassign to the original dataframe, or create a new one?

In the example above, we are reassigning the `lifts` dataframe back to itself after applying the filtering/mutating functions (i.e., **`lifts <- lifts`**`%>% *FUNCTIONS THAT MODIFY THE DATA FRAME*`).

Generally this is an OK thing to do, but you want to be sure that you don't remove or replace information in the original dataframe that you might need later. For example, if we update the `lifts` dataframe to remove non-USA observations, but later we realize we do want to analyze those observations, we will need to go back to reload the raw data and start over.

An alternative when using these functions is to assign the output to a dataframe with a different name. For example, we could assign our filtered data to a dataframe named `lifts_usa` instead, preserving the original `lifts` dataframe if we need to go back to it.

The only catch is that if you create too many dataframes, R may eventually run out of memory and crash (though this shouldn't be a concern with the size of datasets we are typically working with).
:::

## Exercise 3: Get to know the outcome/response variable

Let's get acquainted with the `SWR` variable.

-   Create a new code chunk and use `ggplot` to construct an appropriate plot to visualize the distribution of this variable

-   Create a new code chunk and compute at least two appropriate numerical summaries for the `SWR` variable.

-   Write a brief paragraph interpreting the plot and numerical summaries.

## Exercise 4: Data visualization - two quantitative variables

We'd like to visualize the relationship between body weight and the strength-to-weight ratio. A **scatterplot** (or informally, a "point cloud") allows us to do this! The code below creates a scatterplot of body weight vs. SWR using `ggplot()`.

```{r}
# scatterplot

# The alpha = 0.5 in geom_point() adds transparency to the points
# to make them easier to see. You can decrease this value to make points
# more transparent, and increase it to make them more opaque
lifts %>%
  ggplot(aes(x = BodyweightKg, y = SWR)) +
  geom_point(alpha = 0.5)
```

a.  This is your first **bivariate** data visualization (visualization for two variables)! What differences do you notice in the code structure when creating a bivariate visualization, compared to univariate visualizations we've worked with before?
b.  What similarities do you notice in the code structure?
c.  Does there appear to be some sort of **pattern** in the structure of the point cloud? Describe it, in no more than three sentences! Comment on the direction of the relationship between the two variables (positive? negative?) and the spread of the points (are they dispersed? close together?).

## Exercise 5: Scatterplots - patterns in point clouds

Sometimes, it can be easier to see a pattern in a point cloud by adding a **smoothing** line to our scatterplots. We can accomplish this by adding another layer to the code in Exercise 4:

```{r}
# scatterplot with smoothing line
lifts %>%
  ggplot(aes(x = BodyweightKg, y = SWR)) +
  geom_point(alpha = 0.5) +
  geom_smooth()
```

a.  Look back at your answer to Exercise 4 (c). Does the smoothing line assist you in seeing a pattern, or change your answer at all? Why or why not?
b.  Based on the scatterplot with the smoothing line added above, does there appear to be a **linear** relationship between body weight and SWR (i.e. would a straight line do a decent job at summarizing the relationship between these two variables)? Why or why not?

## Exercise 6: Correlation

We can quantify the **linear** relationship between two quantitative variables using a numerical summary known as **correlation** (sometimes known as a "correlation coefficient" or "Pearson's correlation"). Correlation can range from -1 to 1, where a correlation of 0 indicates that there is *no* linear relationship between the two quantitative variables.

Below is an example of a "Math Box". You'll see these occasionally throughout the activities. You are *not* required to memorize, nor will you be assessed on, anything in the math boxes. If you plan on *continuing* with Statistics courses at Macalester (or are interested in the math behind everything!), these math boxes are for you!

::: {.callout-note collapse="true"}
## Correlation

The Pearson correlation coefficient, $r_{x, y}$, of $x$ and $y$ is the (almost) **average** of products of the z-scores of variables $x$ and $y$:

$$
r_{x, y} = \frac{\sum z_x z_y}{n - 1}
$$
:::

In general, we will want to be able to describe (qualitatively) two aspects of correlation:

1.  Strength

-   Is the correlation between x and y **strong**, or **weak**, i.e. how closely do the points fit around a line? This has to do with how **dispersed** our point clouds are.

2.  Direction

-   Is the correlation between x and y **positive** or **negative**, i.e. does y go "up" when x goes "up" \[positive\], or does y go "down" when x goes "up" (or vice versa) \[negative\]?

Stronger correlations will be *further* from 0 (closer to -1 or 1), and *positive* and *negative* correlations will have the appropriate respective sign (above or below zero).

a.  In addition to a smooth trend line, we can add a linear trendline using `geom_smooth(method = 'lm')`, as below:

```{r}
# scatterplot with linear trend line
lifts %>%
  ggplot(aes(x = BodyweightKg, y = SWR)) +
  geom_point(alpha = 0.5) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    geom_smooth(color = "blue", se = FALSE)
```

b.  Based on the above scatterplot, how would you describe the correlation between body weight and SWR, in terms of strength and direction?
c.  Make a guess as to what *numerical value* the correlation between body weight and SWR will have, based on your response to part (b).

## Exercise 7: Computing correlation in R

We can compute the correlation between body weight and SWR using `summarize` and `cor` functions:

```{r}
# correlation

# Note: the order in which you put your two quantitative variables into the cor
# function doesn't matter! Try switching them around to confirm this for yourself
# Because of the missing data, we need to include the use = "complete.obs" - otherwise the correlation would be computed as NA
lifts %>%
    summarize(cor(SWR, BodyweightKg, use = "complete.obs"))
```

Is the computed correlation close to what you guessed in Exercise 6 part (c)?

## Exercise 8: Model fitting and coefficient interpretation

Let's fit a simple linear regression model and examine the results. Step through the code chunks slowly, and make note of new code.

```{r}
# Construct and save the model as lifts_mod
# the lm() function has two required arguments:
#  - What's the purpose of "SWR ~ BodyweightKg"?
#  - What's the purpose of "data = lifts"?
lifts_mod <- lm(SWR ~ BodyweightKg, data = lifts)
```

```{r}
# The summary() function gives us a detailed glance at the model stored in lifts_mod
summary(lifts_mod)
```

```{r}
# A simplified model summary of just the coefficients
coef(summary(lifts_mod))
```

a.  Using the model summary output, complete the following model formula:\
    E\[SWR \| BodyweightKg\] = \_\_\_ + \_\_\_ \* BodyweightKg

b.  Interpret the intercept in terms of the data context. *Make sure to use non-causal language, include units, and talk about averages rather than individual cases.* Is the intercept meaningful in this situation?

c.  Interpret the slope in terms of the data context. *Make sure to use non-causal language, include units, and talk about averages rather than individual cases.*

## Exercise 9: Predictions and residuals

Let's look at how well our model does at predicting the strength-weight ratio for a lifter named "Chris Della Fave." We can get the relevant **observed** data for Della Fave using the `filter()` and `select()` `dplyr` functions. Noteâ€“but don't worry aboutâ€“the syntax for the `select()` function, we haven't learned this yet:

```{r}
lifts %>% 
    filter(Name == "Chris Della Fave") %>% 
    select(BodyweightKg, SWR) 
```

a.  Peek back at the scatterplot you made in exercise 6a. Identify which point corresponds to this lifter. Is it close to the trend? Is their SWR *higher* or *lower* than expected?

b.  Use your model formula from the previous exercise to find what our model *predicts* Della Fave's SWR should be, based on his body weight. (That is, where do individuals with similar body weight fall on the model trend line? What SWR should would we *expect* from a 118.4kg lifter?)

c.  Check your part b calculation using the `predict()` function. Take careful note of the syntax -- there's a lot going on!

```{r}
# What is the purpose of newdata = ___???
predict(lifts_mod, newdata = data.frame(BodyweightKg = 118.39))
```

d.  Calculate the **residual** or **prediction error**. How far does Della Fave's *observed* SWR fall from the *model prediction*?

    residual = observed y - predicted y = ???

e.  Are positive residuals above or below the trend line? When we have positive residuals, does the model over- or under-estimate SWR? Repeat these questions for negative residuals.

# Additional Practice

## Exercise 10: Lines of best fit

In this activity, we've learned how to fit straight lines to data, to help us visualize the relationship between two quantitative variables. So far, `ggplot` has chosen the line for us. How does it know which line is "best", and what does "best" even mean?

For this exercise, we'll be working with the `anscombe` dataset, which is built in to R. To load this dataset into our environment, we run the following code:

```{r}
# load anscombe data
data("anscombe")
```

We'll consider the relationship between the `x1` and `y1` variables in the `anscombe` dataset. Run the following code, which creates a scatterplot with a fitted line to our data using the function `geom_abline`:

```{r}
# scatterplot with a fitted line, whose slope is 0.4 and intercept is 3
anscombe %>%
  ggplot(aes(x = x1, y = y1)) +
  geom_point() +
  geom_abline(slope = 0.4, intercept = 3, col = "blue", size = 1)
```

Describe the line that you see. Do you think the line is "good"? What are you using to define "good"?

Some things to think about:

-   How many points are **above** the line?
-   How many points are **below** the line?
-   Are the **distances** of the points above and below the line roughly similar, or is there meaningful difference?

Now we'll add *another* line to our plot. Which line do you think is *better* suited for this data? Why? Be specific!

```{r}
# scatterplot with a fitted line, whose slope is 0.4 and intercept is 3
anscombe %>%
  ggplot(aes(x = x1, y = y1)) +
  geom_point() +
  geom_abline(slope = 0.4, intercept = 3, col = "blue", size = 1) +
  geom_abline(slope = 0.5, intercept = 4, col = "orange", size = 1)
```

It's usually quite simple to note when a line is *bad*, but more difficult to quantify when a line is a *good* fit for our data. Consider the following line:

```{r}
# scatterplot with a fitted line, whose slope is 0.4 and intercept is 3
anscombe %>%
  ggplot(aes(x = x1, y = y1)) +
  geom_point() +
  geom_abline(slope = -0.5, intercept = 10, col = "red", size = 1) 
```

In the reading/videos for today, we formalized the **principle of least squares**, which gives us one particular definition of a *line of best fit* that is commonly used in statistics! We'll take advantage of the vertical distances between each point and the fitted line (**residuals**), which will help us define (mathematically) a line that best fits our data:

```{r}
library(broom)
anscombe %>%
  lm(y1 ~ x1, data = .) %>%
  augment() %>%
  ggplot(aes(x = x1, y = y1)) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_segment(aes(xend = x1, yend = .fitted), col = "red") +
  geom_point()
```

## Exercise 11: Data drills (`filter`, `select`, `summarize`)

This exercise is designed to help you keep building your `dplyr` skills. These skills are important to data cleaning and digging, which in turn is important to really making meaning of our data. We'll work with a simpler set of 10 data points:

```{r}
new_lifts <- lifts %>% 
    select(Name, State, Sex, Age, BodyweightKg, SWR) %>% 
    head(10)
```

### Verb 1: `summarize`

Thus far, in the `dplyr` grammar you've seen 3 **verbs** or action words: `summarize()`, `select()`, `filter()`. Try out the following code and then summarize the point of the `summarize()` function:

```{r}
new_lifts %>% 
    summarize(mean_bodyweight = mean(BodyweightKg, na.rm=T),
              mean_SWR = mean(SWR, na.rm=T))
```

### Verb 2: `select`

Try out the following code and then summarize the point of the `select()` function:

```{r}
new_lifts %>%
    select(Name, State)
```

```{r}
new_lifts %>% 
    select(-Name, -State)
```

### Verb 3: `filter`

Try out the following code and then summarize the point of the `filter()` function:

```{r}
new_lifts %>% 
    filter(Age >=28)
```

```{r}
new_lifts %>% 
    filter(State == "CA")
```

```{r}
new_lifts %>% 
    filter(Age >= 28, State=="CA")
```

Use `dplyr` verbs to complete each task below using the `new_lifts` dataframe.

```{r}
# Keep only Bodyweight and SWR variables

# Keep only Bodyweight and SWR variables using a different approach

# Keep only participants (observations) who are female

# Keep only participants (observations) who are female and younger than 30

# Calculate the maximum and minimum ages of participants

```

## Exercise 12: Limitations of correlation

We previously noted that correlation was a numerical summary of the **linear** relationship between two variables. We'll now go through some examples of relationships between quantitative variables to demonstrate why it is *incredibly* important to visualize our data *in addition to* just computing numerical summaries!

We'll retun to the `anscombe` dataset, which contains four different pairs of quantitative variables:

-   `x1`, `y1`
-   `x2`, `y2`
-   `x3`, `y3`
-   `x4`, `y4`

Adapt the code we used in Exercise 7 to compute the correlation between each of these four pairs of variables, below:

```{r}
# correlation between x1, y1

# correlation between x2, y2

# correlation between x3, y3

# correlation between x4, y4

```

a.  What do you notice about each of these correlations (if the answer to this isn't obvious, double-check your code)?

b.  Describe these correlations in terms of strength and direction, using only the numerical summary to assist you in your description.

c.  Draw an example on the white board or at your tables of what you **think** the point clouds for these pairs of variables might look like. There are only 11 observations, so you can draw all 11 points if you'd like!

d.  Adapt the code for scatterplots given previously in this activity to make *four* distinct scatterplots for each pair of quantitative variables in the `anscombe` dataset. You do not need to add a smooth trend line or a linear trend line to these plots.

```{r}
# scatterplot: x1, y1

# scatterplot: x2, y2

# scatterplot: x3, y3

# scatterplot: x4, y4

```

e.  Based on the correlations you calculated and scatterplots you made, what is the *message* of this last exercise as it relates to the limits of correlation?\

## Exercise 13: Correlation and **extreme** values

In this exercise, we'll explore how correlation changes with the addition of **extreme** values, or observations. We'll begin by generating a *simulated* dataset called `dat` with two quantitative variables, `x` and `y`. Run the code below to create the dataset.

**while not required, recall that you can look up function documentation in R using the `?` in front of a function name to figure out what that function is doing!**

```{r}
# create a toy dataset
set.seed(1234)
x <- rnorm(100, mean = 5, sd = 2)
y <- -3 * x + rnorm(100, sd = 4)
dat <- data.frame(x = x, y = y)
```

a.  Make a scatterplot of `x` vs. `y`.

```{r}
# scatterplot

```

b.  Based on your scatterplot, describe the correlation between `x` and `y` in terms of strength and direction.
c.  Guess the correlation (the numerical value) between `x` and `y`.
d.  Compute the correlation between `x` and `y`. Was your guess from part (c) close?

```{r}
# correlation

```

e.  Suppose we observe an additional observation with `x = 15` and `y = -45`. We can create a new data frame, `dat_new1`, that contains this observation in addition to the original ones as follows:

```{r}
# creating dat_new1
x1 <- c(x, 15)
y1 <- c(y, -45)
dat_new1 <- data.frame(x = x1, y = y1)
```

f.  Make a scatterplot of `x` vs. `y` for this new data frame, and compute the correlation between `x` and `y`. Did your correlation change very much with the addition of this observation? Hypothesize why or why not.

```{r}
# scatterplot

# correlation
```

g.  Suppose instead of our additional observation having values `x = 15` and `y = -45`, we instead observe `x = 15` and `y = -15`. We can create a new data frame, `dat_new2`, that contains this observation in addition to the original ones as follows:

```{r}
# creating dat_new1
x2 <- c(x, 15)
y2 <- c(y, 45)
dat_new2 <- data.frame(x = x2, y = y2)
```

h.  Make a scatterplot of `x` vs. `y` for this new data frame, and compute the correlation between `x` and `y`. Did your correlation change very much with the addition of *this* observation? Hypothesize why or why not.

```{r}
# scatterplot

# correlation
```

i.  What do you think the takeaway message is of this exercise?

<!-- -->

j.  **Challenge** Add linear trend lines to your scatterplots from parts (f) and (h). Does this give you any additional insight into why the correlations may have changed in different ways with the addition of a new observation?

## Reflection

Much of statistics is about making (hopefully) reasonable assumptions in attempt to summarize observed relationships in data. Today we started considering assumptions of *linear* relationships between quantitative variables.

Review the learning objectives at the top of this file and today's activity. How do you imagine assumptions of linearity might be useful in terms of quantifying relationships between quantitative variables? How do you imagine these assumptions could sometimes fall short, or even be unethical in certain cases?

> **Response:** Put your response here.

<br><br><br><br><br><br>

[Solutions](../solutions/03_slr_introduction.qmd)